# Awesome Face Reenactment/Talking Face Generation

Papers about FaceSwap in Deepfake are also summarized in the repo [
awesome-faceSwap](https://github.com/DaddyJin/awesome-faceSwap).

## Survey

- <a name="todo"></a> The Creation and Detection of Deepfakes: A Survey (**arXiv 2020**) [[paper](https://arxiv.org/abs/2004.11138)] 
- <a name="todo"></a> DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection (**arXiv 2020**) [[paper](https://arxiv.org/abs/2001.00179)]
- <a name="todo"></a> A Review on Face Reenactment Techniques (**I4Tech 2020**) [[paper](https://ieeexplore.ieee.org/document/9102668)]
- <a name="todo"></a> What comprises a good talking-head video generation?: A Survey and Benchmark (**arXiv 2020**) [[paper](https://arxiv.org/abs/2005.03201)] 
- <a name="todo"></a> Deep Audio-Visual Learning: A Survey (**arXiv 2020**) [[paper](http://arxiv.org/abs/2001.04758)] 
- <a name="todo"></a> Critical review of human face reenactment methods (**JIG 2022**) (Chinese)[[paper](http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?file_no=20220906)] 






## Talking Face Generation Papers

### 2023
- <a name="todo"></a> High-fidelity Generalized Emotional Talking Face Generation
with Multi-modal Emotion Space Learning (**ICCV, 2023**) [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_High-Fidelity_Generalized_Emotional_Talking_Face_Generation_With_Multi-Modal_Emotion_Space_CVPR_2023_paper.pdf)]
- <a name="todo"></a> EMMN: Emotional Motion Memory Network for Audio-driven
Emotional Talking Face Generation(**ICCV, 2023**) [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Tan_EMMN_Emotional_Motion_Memory_Network_for_Audio-driven_Emotional_Talking_Face_ICCV_2023_paper.html)]
- <a name="todo"></a> Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation (**ICCV, 2023**) [[paper](http://arxiv.org/abs/2309.04946)] [[code](https://github.com/yuangan/EAT_code)]
- <a name="todo"></a> Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors (**ICCV, 2023**) [[paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Talking_Head_Generation_with_Probabilistic_Audio-to-Visual_Diffusion_Priors_ICCV_2023_paper.pdf)] 
- <a name="todo"></a> DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation (**CVPR, 2023**) [[paper](https://arxiv.org/abs/2301.03786)] [[code](https://github.com/sstzal/DiffTalk)]
  - <a name="todo"></a> Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis (**CVPR, 2023**) [[paper](https://arxiv.org/abs/2211.14506)] [[webpage](https://dorniwang.github.io/PD-FGC/)] 
- <a name="todo"></a> One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field (**CVPR, 2023**) [[paper](https://arxiv.org/abs/2304.05097)] [[webpage](https://waytron.net/hidenerf/)] 
- <a name="todo"></a> Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert (**CVPR, 2023**) [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Seeing_What_You_Said_Talking_Face_Generation_Guided_by_a_CVPR_2023_paper.pdf)] 
- <a name="todo"></a> LipFormer: High-Fidelity and Generalizable Talking Face Generation With a Pre-Learned Facial Codebook (**CVPR, 2023**) [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LipFormer_High-Fidelity_and_Generalizable_Talking_Face_Generation_With_a_Pre-Learned_CVPR_2023_paper.pdf)] 
- <a name="todo"></a> High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning (**CVPR, 2023**) [[paper](https://arxiv.org/abs/2305.02572)] 
- <a name="todo"></a> OTAvatar : One-shot Talking Face Avatar with Controllable Tri-plane Rendering (**CVPR, 2023**) [[paper](https://arxiv.org/abs/2303.14662)]  [[code](https://github.com/theEricMa/OTAvatar)]
- <a name="todo"></a> IP_LAP: Identity-Preserving Talking Face Generation with Landmark and Appearance Priors  (**CVPR, 2023**)  [[code](https://github.com/Weizhi-Zhong/IP_LAP)]
- <a name="todo"></a> SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation (**CVPR, 2023**) [[paper](https://arxiv.org/abs/2211.12194)] [[webpage](https://sadtalker.github.io/)] [[code](https://github.com/Winfredy/SadTalker)]
- <a name="todo"></a> DPE: Disentanglement of Pose and Expression for General Video Portrait Editing (**CVPR, 2023**) [[paper](https://arxiv.org/abs/2301.06281)] [[webpage](https://carlyx.github.io/DPE/)] [[code](https://github.com/Carlyx/DPE)]
- <a name="todo"></a>  StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles (**AAAI, 2023**) [[paper](https://arxiv.org/pdf/2301.01081.pdf)] [[code](https://github.com/FuxiVirtualHuman/styletalk)]
- <a name="todo"></a>  DINet: Deformation Inpainting Network for Realistic Face Visually Dubbing on High Resolution Video (**AAAI, 2023**) [[paper](https://fuxivirtualhuman.github.io/pdf/AAAI2023_FaceDubbing.pdf)] [[code](https://github.com/MRzzm/DINet)]
- <a name="todo"></a> Audio-Visual Face Reenactment (**WACV, 2023**) [[paper](https://arxiv.org/abs/2210.02755)] [[webpage](http://cvit.iiit.ac.in/research/projects/cvit-projects/avfr)] [[code](https://github.com/mdv3101/AVFR-Gan/)]
- <a name="todo"></a> Emotionally Enhanced Talking Face Generation (**Arxiv, 2023**) [[paper](https://arxiv.org/abs/2303.11548)] [[webpage](https://midas.iiitd.edu.in/emo/)] [[code](https://github.com/sahilg06/EmoGen)]
- <a name="todo"></a> Compact Temporal Trajectory Representation for
Talking Face Video Compression (**TCSVT, 2023**) [[paper](https://ieeexplore.ieee.org/abstract/document/10109861/)] 


### 2022
- <a name="todo"></a>  VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild (**SIGGRAPH ASIA, 2022**) [[paper](https://arxiv.org/abs/2211.14758)] [[code]](https://github.com/vinthony/video-retalking)
- <a name="todo"></a>  Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis (**ECCV, 2022**) [[paper](https://github.com/sstzal/DFRF/blob/show_page/images/DFRF_eccv2022.pdf)] [[code](https://github.com/sstzal/DFRF)]
- <a name="todo"></a> Compressing Video Calls using Synthetic Talking Heads (**BMVC, 2022**) [[paper](https://arxiv.org/abs/2210.03692)] [[webpage](https://cvit.iiit.ac.in/research/projects/cvit-projects/talking-video-compression)] 
- <a name="todo"></a>  Expressive Talking Head Generation With Granular Audio-Visual Control (**CVPR, 2022**) [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Expressive_Talking_Head_Generation_With_Granular_Audio-Visual_Control_CVPR_2022_paper.pdf)] 
- <a name="todo"></a>  EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model (**SIGGRAPH, 2022**) [[paper](http://arxiv.org/abs/2205.15278)] 
- <a name="todo"></a>  Emotion-Controllable Generalized Talking Face Generation (**IJCAI, 2022**) [[paper](http://arxiv.org/abs/2205.01155)] 
- <a name="todo"></a>  StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pretrained StyleGAN (**ECCV, 2022**) [[paper](https://arxiv.org/pdf/2203.04036.pdf)] 
- <a name="todo"></a>  SyncTalkFace: Talking Face Generation with Precise Lip-syncing via Audio-Lip Memory (**AAAI, 2022**) [[paper](https://www.aaai.org/AAAI22Papers/AAAI-7528.ParkS.pdf)] 
- <a name="todo"></a> One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning (**AAAI, 2022**) [[paper](http://arxiv.org/abs/2112.02749)] 
- <a name="todo"></a> Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels (**TMM, 2022**) [[paper](http://arxiv.org/abs/2201.05986)] 
- <a name="todo"></a> Audio-driven Dubbing for User Generated Contents via Style-aware Semi-parametric Synthesis (**TCSVT, 2022**) [[paper](https://ieeexplore.ieee.org/abstract/document/9903679/)] 



### 2021
- <a name="todo"></a> Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation (**SIGGRAPH ASIA, 2021**) [[paper](http://arxiv.org/abs/2109.10595)] 
- <a name="todo"></a> Imitating Arbitrary Talking Style for Realistic Audio-DrivenTalking Face Synthesis (**MM, 2021**) [[paper](http://arxiv.org/abs/2111.00203)] [[code](https://github.com/wuhaozhe/style_avatar)]
- <a name="todo"></a> Towards Realistic Visual Dubbing with Heterogeneous Sources (**MM, 2021**) [[paper](https://dl.acm.org/doi/abs/10.1145/3474085.3475318)]
- <a name="todo"></a> Talking Head Generation with Audio and Speech Related Facial Action Units (**BMVC, 2021**) [[paper](http://arxiv.org/abs/2110.09951)] 
- <a name="todo"></a> 3D Talking Face with Personalized Pose Dynamics (**TVCG, 2021**) [[paper](https://personal.utdallas.edu/~xxg061000/TVCG2021.pdf)] 
- <a name="todo"></a> Talking Head Generation with Audio and Speech Related Facial Action Units (**BMVC, 2021**) [[paper](http://arxiv.org/abs/2110.09951)] 
- <a name="todo"></a> FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning (**ICCV, 2021**) [[paper](http://arxiv.org/abs/2108.07938)] [[code](https://github.com/zhangchenxu528/FACIAL)]
- <a name="todo"></a> AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis (**ICCV, 2021**) [[paper](http://arxiv.org/abs/2103.11078)] [[code](https://github.com/YudongGuo/AD-NeRF)]
- <a name="todo"></a> Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion (**IJCAI, 2021**) [[paper](http://arxiv.org/abs/2107.09293)] 
- <a name="todo"></a> Flow-Guided One-Shot Talking Face Generation With a High-Resolution Audio-Visual Dataset (**CVPR, 2021**) [[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Flow-Guided_One-Shot_Talking_Face_Generation_With_a_High-Resolution_Audio-Visual_Dataset_CVPR_2021_paper.pdf)] 
- <a name="todo"></a> Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation (**CVPR, 2021**) [[paper](https://arxiv.org/abs/2104.11116)] [[code](https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS)]
- <a name="todo"></a> Audio-Driven Emotional Video Portraits (**CVPR, 2021**) [[paper](http://arxiv.org/abs/2104.07452)] [[code](https://github.com/jixinya/EVP)]
- <a name="todo"></a> Everything's Talkin': Pareidolia Face Reenactment (**CVPR, 2021**) [[paper](http://arxiv.org/abs/2104.03061)] 
- <a name="todo"></a> APB2FaceV2: Real-Time Audio-Guided Multi-Face Reenactment (**ICASSP, 2021**) [[paper](https://arxiv.org/abs/2010.13017v1)] [[code](https://github.com/zhangzjn/APB2FaceV2)]

### 2020
- <a name="todo"></a> Talking-head Generation with Rhythmic Head Motion (**ECCV, 2020**) [[paper](https://arxiv.org/abs/2007.08547)] [[code]([https://github.com/uniBruce/Mead](https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion))]
- <a name="todo"></a> MEAD: A Large-Scale Audio-Visual Dataset for Emotional Talking-Face Generation (**ECCV, 2020**) [[paper](http://link.springer.com/10.1007/978-3-030-58589-1_42)] [[code](https://github.com/uniBruce/Mead)]
- <a name="todo"></a> A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild (**MM, 2020**) [[paper](http://arxiv.org/abs/2008.10010)] [[code](https://github.com/Rudrabha/Wav2Lip)]
- <a name="todo"></a> Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence Learning (**IJCAI, 2020**) [[paper](http://arxiv.org/abs/1812.06589)] 
- <a name="todo"></a> APB2Face: Audio-guided face reenactment with auxiliary pose and blink signals (**ICASSP, 2020**) [[paper](https://arxiv.org/abs/2004.14569v1)] [[code](https://github.com/zhangzjn/APB2Face)]
- <a name="todo"></a> MakeItTalk: Speaker-Aware Talking Head Animation (**SIGGRAPH ASIA, 2020**) [[paper](http://arxiv.org/abs/2004.12992)] [[code](https://github.com/yzhou359/MakeItTalk)]
- <a name="todo"></a> Everybody’s Talkin’: Let Me Talk as You Want (**arXiv, 2020**) [[paper](http://arxiv.org/abs/2001.05201)] 
- <a name="todo"></a> Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose (**arXiv, 2020**) [[paper]([http://arxiv.org/abs/2001.05201](http://arxiv.org/abs/2002.10137))]  [[code](https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose)]
- <a name="todo"></a> Multimodal inputs driven talking face generation with spatial–temporal dependency (**TCSVT, 2020**) [[paper](https://ieeexplore.ieee.org/abstract/document/8995571/)] 


### 2019
- <a name="todo"></a> Talking Face Generation by Adversarially Disentangled Audio-Visual Representation(**AAAI, 2019**) [[paper](http://arxiv.org/abs/1807.07860)]
- <a name="todo"></a> Towards Automatic Face-to-Face Translation (**MM, 2019**) [[paper](http://arxiv.org/abs/2003.00418)] [[code](https://github.com/Rudrabha/LipGAN)]
- <a name="todo"></a> Few-Shot Adversarial Learning of Realistic Neural Talking Head Models (**ICCV, 2019**) [[paper](http://arxiv.org/abs/1905.08233)] 
- <a name="todo"></a> Learning the Face Behind a Voice (**CVPR, 2019**) [[paper](https://ieeexplore.ieee.org/document/8953196/)] [[code](https://github.com/saiteja-talluri/Speech2Face)]
- <a name="todo"></a> Hierarchical Cross-Modal Talking Face Generation with Dynamic Pixel-Wise Loss (**CVPR, 2019**) [[paper](http://arxiv.org/abs/1905.03820)] [[code](https://github.com/lelechen63/ATVGnet)]
- <a name="todo"></a> Wav2Pix: Speech-conditioned Face Generation using Generative Adversarial Networks (**ICASSP, 2019**) [[paper](http://arxiv.org/abs/1903.10195)] [[code](https://github.com/miqueltubau/Wav2Pix)]
- <a name="todo"></a> Face Reconstruction from Voice using Generative Adversarial Networks (**NIPS, 2019**) [[paper](https://papers.nips.cc/paper/8768-face-reconstruction-from-voice-using-generative-adversarial-networks.pdf)] 
- <a name="todo"></a> Talking Face Generation by Conditional Recurrent Adversarial Network (**IJCAI, 2019**) [[paper]([https://papers.nips.cc/paper/8768-face-reconstruction-from-voice-using-generative-adversarial-networks.pdf](https://arxiv.org/pdf/1804.04786.pdf))] [[code](https://github.com/susanqq/Talking_Face_Generation)]









## Face Reenactment Papers

### 2024
- <a name="todo"></a> 3D-Aware Talking-Head Video Motion Transfer (**WACV, 2024**) [[paper](https://openaccess.thecvf.com/content/WACV2024/papers/Ni_3D-Aware_Talking-Head_Video_Motion_Transfer_WACV_2024_paper.pdf)]

### 2023
- <a name="todo"></a> High-Fidelity and Freely Controllable Talking Head Video Generation (**CVPR, 2023**) [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_High-Fidelity_and_Freely_Controllable_Talking_Head_Video_Generation_CVPR_2023_paper.pdf)]
- <a name="todo"></a> MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation (**CVPR, 2023**) [[paper](https://arxiv.org/abs/2212.08062)] [[webpage](https://meta-portrait.github.io/)] [[code](https://github.com/Meta-Portrait/MetaPortrait)]
- <a name="todo"></a> HR-Net: a landmark based high realistic face reenactment network (**TCSVT, 2023**) [[paper](https://ieeexplore.ieee.org/abstract/document/10103929/)]



### 2022
- <a name="todo"></a> Dual-Generator Face Reenactment (**CVPR, 2022**) [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Hsu_Dual-Generator_Face_Reenactment_CVPR_2022_paper.pdf)] 
- <a name="todo"></a> Depth-Aware Generative Adversarial Network for Talking Head Video Generation (**CVPR, 2022**) [[paper](https://arxiv.org/abs/2203.06605)] 
- <a name="todo"></a> Latent Image Animator: Learning to Animate Images via Latent Space Navigation (**ICLR, 2022**) [[paper](https://openreview.net/pdf?id=7r6kDq0mK_)] 
- <a name="todo"></a> Finding Directions in GAN’s Latent Space for Neural Face Reenactment (**BMVC, 2022**) [[paper](https://arxiv.org/pdf/2202.00046.pdf)][[code](https://github.com/StelaBou/stylegan_directions_face_reenactment)]
- <a name="todo"></a> FSGANv2: Improved Subject Agnostic Face Swapping and Reenactment (**PAMI, 2022**) [[paper](http://arxiv.org/abs/2202.12972)] 

### 2021
- <a name="todo"></a> PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering (**ICCV, 2021**) [[paper](http://arxiv.org/abs/2109.08379)] 
- <a name="todo"></a> LI-Net: Large-Pose Identity-Preserving Face Reenactment Network (**ICME, 2021**) [[paper](https://arxiv.org/pdf/2104.02850)] 
- <a name="todo"></a> One-shot Face Reenactment Using Appearance Adaptive Normalization (**AAAI, 2021**) [[paper](https://arxiv.org/pdf/2102.03984.pdf)] 
- <a name="todo"></a> A unified framework for high fidelity face swap and expression reenactment (**TCSVT, 2021**) [[paper](https://ieeexplore.ieee.org/abstract/document/9517088/)]

### 2020
- <a name="todo"></a> One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing (**arXiv, 2020**) [[paper](http://arxiv.org/abs/2011.15126)] 
- <a name="todo"></a> FACEGAN: Facial Attribute Controllable rEenactment GAN (**WACV, 2020**) [[paper](http://arxiv.org/abs/2011.04439)] 
- <a name="todo"></a> LandmarkGAN: Synthesizing Faces from Landmarks (**arXiv, 2020**) [[paper](http://arxiv.org/abs/2011.00269)] 
- <a name="todo"></a> Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars (**ECCV, 2020**) [[paper](http://arxiv.org/abs/2008.10174)] [[code](https://github.com/saic-violet/bilayer-model)]
- <a name="todo"></a> Mesh Guided One-shot Face Reenactment using Graph Convolutional Networks (**MM, 2020**) [[paper](http://arxiv.org/abs/2008.07783)] 
- <a name="todo"></a> Learning Identity-Invariant Motion Representations for Cross-ID Face Reenactment (**CVPR, 2020**) [[paper](http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Learning_Identity-Invariant_Motion_Representations_for_Cross-ID_Face_Reenactment_CVPR_2020_paper.pdf)] 
- <a name="todo"></a> ReenactNet: Real-time Full Head Reenactment (**arXiv, 2020**) [[paper](http://arxiv.org/abs/2006.10500)] 
- <a name="todo"></a> FReeNet: Multi-Identity Face Reenactment (**CVPR, 2020**) [[paper](http://arxiv.org/abs/1905.11805)] [[code](https://github.com/zhangzjn/FReeNet)]
- <a name="todo"></a> FaR-GAN for One-Shot Face Reenactment (**CVPRW, 2020**) [[paper](http://arxiv.org/abs/2005.06402)] 
- <a name="todo"></a> One-Shot Identity-Preserving Portrait Reenactment (**, 2020**) [[paper](http://arxiv.org/abs/2004.12452)] 
- <a name="todo"></a> Neural Head Reenactment with Latent Pose Descriptors (**CVPR, 2020**) [[paper](http://arxiv.org/abs/2004.12000)] [[code](https://github.com/shrubb/latent-pose-reenactment)]
- <a name="todo"></a> ActGAN: Flexible and Efficient One-shot Face Reenactment (**IWBF, 2020**) [[paper](http://arxiv.org/abs/2003.13840)] 
- <a name="todo"></a> Realistic Face Reenactment via Self-Supervised Disentangling of Identity and Pose (**AAAI, 2020**) [[paper](http://arxiv.org/abs/2003.12957)] 
- <a name="todo"></a> First Order Motion Model for Image Animation (**NIPS, 2020**) [[paper](http://arxiv.org/abs/2003.00196)] [[code](https://github.com/AliaksandrSiarohin/first-order-model)]

### 2019
- <a name="todo"></a> FLNet: Landmark Driven Fetching and Learning Network for Faithful Talking Facial Animation Synthesis (**AAAI, 2019**) [[paper](http://arxiv.org/abs/1911.09224)] 
- <a name="todo"></a> MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets (**AAAI, 2019**) [[paper](http://arxiv.org/abs/1911.08139)] 
- <a name="todo"></a> Any-to-one Face Reenactment Based on Conditional Generative Adversarial Network (**APSIPA, 2019**) [[paper](https://ieeexplore.ieee.org/document/9023328/)] 
- <a name="todo"></a> Make a Face: Towards Arbitrary High Fidelity Face Manipulation (**ICCV, 2019**) [[paper](http://arxiv.org/abs/1908.07191)] 
- <a name="todo"></a> One-shot Face Reenactment (**BMVC, 2019**) [[paper](http://arxiv.org/abs/1908.03251)] [[code](https://github.com/bj80heyue/One_Shot_Face_Reenactment)]
- <a name="todo"></a> Deferred neural rendering: image synthesis using neural textures (**TOG, 2019**) [[paper](https://dl.acm.org/doi/10.1145/3306346.3323035)] 
- <a name="todo"></a> Animating Arbitrary Objects via Deep Motion Transfer (**CVPR, 2019**) [[paper](https://arxiv.org/abs/1812.08861)] [[code](https://github.com/AliaksandrSiarohin/monkey-net)]
- <a name="todo"></a> FSGAN: Subject Agnostic Face Swapping and Reenactment (**ICCV, 2019**) [[paper](http://arxiv.org/abs/1908.05932)] [[code](https://github.com/YuvalNirkin/fsgan)]

### 2018
- <a name="todo"></a> GANimation: Anatomically-aware Facial Animation from a Single Image (**ECCV, 2018**) [[paper](http://arxiv.org/abs/1807.09251)] [[code](https://github.com/albertpumarola/GANimation)]
- <a name="todo"></a> ReenactGAN: Learning to Reenact Faces via Boundary Transfer (**ECCV, 2018**) [[paper](http://arxiv.org/abs/1807.11079)] [[code](https://github.com/wywu/ReenactGAN)]
- <a name="todo"></a> Deep Video Portraits (**SIGGRAPH, 2018**) [[paper](http://arxiv.org/abs/1805.11714)] 
- <a name="todo"></a> X2Face: A Network for Controlling Face Generation Using Images, Audio, and Pose Codes (**ECCV, 2018**) [[paper](http://link.springer.com/10.1007/978-3-030-01261-8_41)] [[code](https://github.com/oawiles/X2Face)]

### 2016

- <a name="todo"></a> Face2Face: Real-time Face Capture and Reenactment of RGB Videos (**CVPR, 2016**) [[paper](http://www.graphics.stanford.edu/~niessner/papers/2016/1facetoface/thies2016face.pdf)]

### 2018
- <a name="todo"></a> Lip movements generation at a glance (**ECCV, 2018**) [[paper](https://arxiv.org/abs/1803.10404)]

### 2017

- <a name="todo"></a> Synthesizing Obama: learning lip sync from audio (**TOG, 2017**) [[paper](https://dl.acm.org/doi/10.1145/3072959.3073640)] 
- <a name="todo"></a> You said that? (**BMVC, 2017**) [[paper](http://arxiv.org/abs/1705.02966)] [[code](https://github.com/joonson/yousaidthat)]












